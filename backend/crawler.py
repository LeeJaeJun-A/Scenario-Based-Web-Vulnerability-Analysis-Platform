import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse


class Crawler:
    def __init__(self, base_url, max_depth=3):
        self.base_url = base_url
        self.max_depth = max_depth
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.input_data = []
        self.link_data = []

    def load_page(self, url):
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Error {url}: {e}")
            return None

    def parsing(self, html, current_url):
        soup = BeautifulSoup(html, 'html.parser')
        base_input = soup.find_all('input')
        base_link = soup.find_all('a')

        input_fin = [(input.get('name'), input.get('type')) for input in base_input if input.get('name')]
        url_fin = [(urljoin(current_url, link.get('href')), link.text.strip()) for link in base_link if link.get('href')]
        return input_fin, url_fin

    def crawl_for_inputs(self, url, depth=0, visited=None):
        if visited is None:
            visited = set()

        if depth > self.max_depth or url in visited:
            return

        visited.add(url)
        html = self.load_page(url)
        if html:
            try:
                input_fin, link_fin = self.parsing(html, url)
                self.input_data.extend(input_fin)
                for input_name, input_type in input_fin:
                    print(f"Found input: name={input_name}, type={input_type}")

                for link_url, _ in link_fin:
                    parsed_link = urlparse(link_url)
                    if parsed_link.netloc == urlparse(self.base_url).netloc:
                        try:
                            self.crawl_for_inputs(link_url, depth + 1, visited)
                        except Exception as e:
                            pass
            except Exception as e:
                pass

    def crawl_for_links(self, url, depth=0, visited=None):
        if visited is None:
            visited = set()

        if depth > self.max_depth or url in visited:
            return

        visited.add(url)
        html = self.load_page(url)
        if html:
            try:
                _, link_fin = self.parsing(html, url)
                self.link_data.extend(link_fin)
                for link_url, link_text in link_fin:
                    print(f"Found link: url={link_url}, text={link_text}")

                for link_url, _ in link_fin:
                    parsed_link = urlparse(link_url)
                    if parsed_link.netloc == urlparse(self.base_url).netloc:
                        try:
                            self.crawl_for_links(link_url, depth + 1, visited)
                        except Exception as e:
                            pass
            except Exception as e:
                pass


if __name__ == "__main__":
    base_url = 'https://mportal.ajou.ac.kr/main.do'
    max_depth = 3
    crawler = Crawler(base_url, max_depth=max_depth)

    crawler.crawl_for_inputs(base_url, depth=0)

    crawler.crawl_for_links(base_url, depth=0)
    print("====================================")

    print("All found input fields:")
    for input_name, input_type in crawler.input_data:
        print(f"Input: name={input_name}, type={input_type}")

    print("\nAll found links:")
    for link_url, link_text in crawler.link_data:
        print(f"Link: url={link_url}, text={link_text}")